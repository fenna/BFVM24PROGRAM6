# Assignment: Assessing the FASTEP principles of Your ML Application

In the healthcare sector, Artificial Intelligence (AI) systems have become transformative tools, powering applications from disease diagnosis and treatment recommendations to patient monitoring and hospital resource optimization. However, the deployment of AI in healthcare comes with unique challenges and responsibilities. Unlike other domains, errors in healthcare AI systems can have direct consequences on patient health and safety, making robustness and trustworthiness paramount.
The Responsible AI (RAI) framework offers a structured approach to ensuring AI systems in healthcare are designed and deployed with the well-being of patients and society at their core. Using the FASTEPS principles—Fairness, Accountability, Safety, Transparency, Ethics, Privacy, and Security—healthcare AI systems can address critical concerns such as mitigating biases in diagnostic tools, ensuring patient data privacy, and providing explainable decisions to support clinical trust.

This assignment focuses on evaluating the robustness of AI systems against these principles. Students will analyze systems to assess their ability to deliver safe, ethical, and reliable outcomes, while maintaining high standards of privacy and security. The goal is to identify gaps and propose actionable solutions. 

## Objective:
Evaluate the robustness of an AI or ML system through the lens of the FASTEPS principles: Fairness, Accountability, Safety, Transparency, Ethics, Privacy, and Security. This assignment challenges students to analyze an AI system's design, development, and deployment processes to identify strengths, weaknesses, and areas for improvement.

## Preparatory Reading:
Familiarize yourself with the concepts of Responsible AI (RAI) and the FASTEPS framework. Lectures on this topic will be provided. Suggested resources (more articles can be found at the bottom of this page): 

Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. FAT* ’19. New York, NY, USA: Association for Computing Machinery, 2019. https://doi.org/10.1145/3287560.3287596. Access via :https://arxiv.org/pdf/1810.03993

Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. “Explainable AI: A Review of Machine Learning Interpretability Methods.” Entropy 23, no. 1 (2021). https://doi.org/10.3390/e23010018 

## System choice:
Choose an AI or ML application to analyze. This can be a system you’ve worked on, or the system you will work on during your internship. This assignment can be done in groups. 

## Asignment
Using the FASTEPS principles, assess the AI system’s robustness as follows:

### Fairness (Inclusiveness):
Analyze how the system performs across various subpopulations (e.g., race, gender, background). Alternatively, assess the data that is or will be used for training.
Identify potential biases in training data, models, or outcomes. Suggest strategies to mitigate these biases.

### Accountability:
Investigate who is accountable for the system's decisions and actions.
Propose a clear accountability process to handle failures or controversial decisions.

### Safety (Reliability):
How are we ensuring that the system is robust, reliable and prioritise the safety of users? Are the stakeholders and organizational needs clear? Are the system requirements clear? How is or will be the Verification &Validation performed on al subcompontent of the system? Is there an end-to-end user validation test? When deployed are SOPs in place to monitor? 
Is the system’s robustness and reliability tested under varying conditions?
Suggest actions to ensure safety.

### Transparency (Explainability):
Assess whether the system provides clear explanations of its data usage and decision-making processes.
Suggest improvements for enhancing user understanding and trust. (See also the references to explainable AI)

### Ethics:
Critique whether the system aligns with moral values and societal expectations.
Identify ethical dilemmas and recommend solutions to address them.

### Privacy:
Assess how the system ensures user data privacy.
Identify risks of misuse or data exploitation and suggest preventive measures.

### Security:
Evaluate the system's resilience to attacks (e.g., data breaches, adversarial attacks).
Propose methods to enhance security and reduce vulnerabilities.

## Deliverables
Provide a 3-4 pages report with the assessment of the system against each FASTEPS principle. Actionable recommendations for improvement should be included. Improvement should be based on references (discussion with system stakeholders and or literature). 

## references:

AI Governance Frameworks
EU AI Act: Focuses on ensuring AI systems comply with ethical and legal standards.
NIST AI Risk Management Framework: Provides guidance on assessing and mitigating risks in AI systems.
Google’s AI Principles: A corporate framework focusing on ethical AI development and use.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems: Offers guidelines for ethically aligned AI design.

Bellamy, R. K. E., K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, et al. “AI Fairness 360: An Extensible Toolkit for Detecting and Mitigating Algorithmic Bias.” IBM Journal of Research and Development 63, no. 4/5 (2019): 4:1-4:15. https://doi.org/10.1147/JRD.2019.2942287.

Byrne, Ruth MJ. “Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning.” In IJCAI, 6276–82. California, CA, 2019.
Chambers, Alicia. “Re: Artificial Intelligence Risk Management Framework,” 2021.

Fjeld, Jessica, Nele Achten, Hannah Hilligoss, Adam Nagy, and Madhulika Srikumar. “Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI.” Berkman Klein Center Research Publication, no. 2020–1 (2020). https://doi.org/https://dx.doi.org/10.2139/ssrn.3518482.

He, Yingzhe, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He. “Towards Security Threats of Deep Learning Systems: A Survey.” IEEE Transactions on Software Engineering 48, no. 5 (2022): 1743–70. https://doi.org/10.1109/TSE.2020.3034721.

Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. “Explainable AI: A Review of Machine Learning Interpretability Methods.” Entropy 23, no. 1 (2021). https://doi.org/10.3390/e23010018.

Martinez, D.R. and Kifle, B.M., 2024. Artificial Intelligence: A Systems Approach from Architecture Principles to Deployment. MIT Press.

Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. FAT* ’19. New York, NY, USA: Association for Computing Machinery, 2019. https://doi.org/10.1145/3287560.3287596.



