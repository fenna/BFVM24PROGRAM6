{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK ML\n",
    "\n",
    "Use dask in case of high computation time or high memory usage. Otherwise it is not an advantage. \n",
    "\n",
    "Many of scikit-learn’s ML functions, including cross-validation, hyperparameter search, clustering, regression, imputation, and scoring methods, have a dask equivalent. You must make sure your data is a data colletion of dask, and if you use `to_array()` you explicit the chunck size. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.591318253692751\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# https://archive.ics.uci.edu/dataset/504/qsar+fish+toxicity\n",
    "# Data set containing values for 6 attributes (molecular descriptors) \n",
    "# of 908 chemicals used to predict quantitative acute aquatic toxicity \n",
    "# towards the fish Pimephales promelas (fathead minnow).\n",
    "\n",
    "path = \"data/qsar_fish_toxicity.csv\"\n",
    "column_names = [\n",
    "    'CIC0', 'SM1_Dz(Z)', 'GATS1i', 'NdsCH', 'NdssC', 'MLOGP', 'LC50'\n",
    "]\n",
    "df = dd.read_csv(path, names=column_names, sep=';')\n",
    "\n",
    "# Prepare the data for regression\n",
    "regr_X = df.drop(columns=['LC50'])\n",
    "regr_y = df[['LC50']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(regr_X, \n",
    "                                                    regr_y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "# Convert to Dask arrays\n",
    "X_train = X_train.to_dask_array(lengths=True)\n",
    "X_test = X_test.to_dask_array(lengths=True)\n",
    "y_train = y_train.to_dask_array(lengths=True)\n",
    "y_test = y_test.to_dask_array(lengths=True)\n",
    "\n",
    "# Train the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Compute the R^2 score\n",
    "score = r2_score(y_test.compute(), y_pred.compute())  # Compute only at evaluation step\n",
    "print(f'R^2 score: {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a function that exists in scikit-learn or other data science libraries but not in Dask-ML, you can use DASK-ML as a wrapper around scikit-learn to make it distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Scores: (0.5920439021777781,)\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# List of estimators\n",
    "estimators = [LinearRegression ()]\n",
    "\n",
    "# Delayed tasks for training\n",
    "train_chunck = [dask.delayed(estimator.fit)(X_train, y_train) for estimator in estimators]\n",
    "\n",
    "# Delayed tasks for prediction\n",
    "predict_chunks= [dask.delayed(estimator.predict)(X_test) for estimator in train_chunck]\n",
    "\n",
    "# Delayed tasks for scoring\n",
    "scores = [dask.delayed(r2_score)(y_test, y_pred) for y_pred in predict_chunks]\n",
    "\n",
    "# Execute the entire pipeline\n",
    "scores = dask.compute(*scores)\n",
    "print(\"R^2 Scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed (Dask Client and Scheduler)\n",
    "\n",
    "Imagine you're solving a challenging problem that requires significant computational power, like tuning a Ridge regression model on a large dataset with complex hyperparameters. While Dask works well locally on a single machine, its true strength lies in its distributed scheduler. By connecting to a Dask cluster, you can scale your computations across multiple machines, whether it's a local cluster or a larger distributed system. This portability allows Dask to adapt to the resources available, efficiently distributing tasks across workers. By this it can handle demanding workloads, making large-scale hyperparameter tuning and other computationally intensive tasks fast, efficient, and scalable. Before you can work with your code you need to initiate a scheduler and one or more workers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Dask scheduler and workers using the dask-scheduler and dask-worker commands in separate terminal windows.\n",
    "```{bash}\n",
    "# Start the Dask scheduler\n",
    "dask-scheduler\n",
    "\n",
    "# In a separate terminal window, start one or more Dask workers\n",
    "dask-worker tcp://127.0.0.1:8786\n",
    "\n",
    "# you can also specify memory and threads\n",
    "dask-worker tcp://127.0.0.1:8786 --nthreads 4 --memory-limit 2GB\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://192.168.2.186:8786' processes=2 threads=12, memory=17.86 GiB>\n",
      "Number of workers: 2\n",
      "Worker: tcp://127.0.0.1:56658\n",
      "  Memory: 17.18 GB\n",
      "  CPU: 8 threads\n",
      "  Tasks: 8 tasks\n",
      "Worker: tcp://127.0.0.1:58338\n",
      "  Memory: 2.00 GB\n",
      "  CPU: 4 threads\n",
      "  Tasks: 4 tasks\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client \n",
    "client = Client('127.0.0.1:8786', timeout=60)  # Increase timeout to 60 seconds\n",
    "#client = Client(nworkers=4)\n",
    "print(client)\n",
    "# Get the status of the workers\n",
    "workers = client.scheduler_info()['workers']\n",
    "print(f\"Number of workers: {len(workers)}\")\n",
    "for worker, info in workers.items():\n",
    "    print(f\"Worker: {worker}\")\n",
    "    print(f\"  Memory: {info['memory_limit'] / 1e9:.2f} GB\")\n",
    "    print(f\"  CPU: {info['nthreads']} threads\")\n",
    "    print(f\"  Tasks: {info['nthreads']} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dask scheduler and workers we now can direct the joblib backend to dask\n",
    "\n",
    "    from joblib import parallel_backend #to use dask as backend\n",
    "    with parallel_backend('dask')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 1e+03 ns, total: 4 μs\n",
      "Wall time: 7.15 μs\n",
      "{'mean_fit_time': array([0.00681005, 0.00672436, 0.00682592]), 'std_fit_time': array([0.00359283, 0.0032681 , 0.00354043]), 'mean_score_time': array([0.00041399, 0.00041456, 0.00044198]), 'std_score_time': array([6.61573710e-05, 5.13300085e-05, 3.99079942e-05]), 'param_alpha': masked_array(data=[0.1, 1.0, 10.0],\n",
      "             mask=[False, False, False],\n",
      "       fill_value=1e+20), 'params': [{'alpha': 0.1}, {'alpha': 1.0}, {'alpha': 10.0}], 'split0_test_score': array([-0.01217785, -0.09662753, -7.84541016]), 'split1_test_score': array([-0.01304742, -0.07148277, -5.926423  ]), 'split2_test_score': array([-0.00964671, -0.06847693, -6.18101719]), 'split3_test_score': array([-0.01071038, -0.06766663, -5.44388354]), 'split4_test_score': array([-0.01060364, -0.07510655, -6.60444719]), 'mean_test_score': array([-0.0112372 , -0.07587208, -6.40023621]), 'std_test_score': array([0.00121411, 0.01070195, 0.81454235]), 'rank_test_score': array([1, 2, 3], dtype=int32)}\n",
      "Mean Squared Error: 0.010580128134313865\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from joblib import parallel_backend #to use dask as backend\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Set up the grid search\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "gs = GridSearchCV(ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform the grid search with Dask parallel backend\n",
    "with parallel_backend('dask'):\n",
    "    gs.fit(X, y)\n",
    "\n",
    "# Print the results\n",
    "print(gs.cv_results_)\n",
    "\n",
    "# Best estimator\n",
    "best_ridge = gs.best_estimator_\n",
    "\n",
    "# Predict on the training data\n",
    "y_pred = best_ridge.predict(X)\n",
    "\n",
    "# Compute the mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST\n",
    "\n",
    "Dask-XGBoost is an extension of the XGBoost library that leverages Dask, a parallel computing framework in Python, to distribute and scale the training of XGBoost models across multiple CPUs or GPUs, machines, or clusters. \n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/tutorials/dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from dask import array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "feature_names = data.feature_names.tolist()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Convert to Dask arrays\n",
    "X = da.from_array(X, chunks=(100, X.shape[1]))\n",
    "y = da.from_array(y, chunks=(100,))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train.compute(), label=y_train.compute(), feature_names=feature_names)\n",
    "dvalid = xgb.DMatrix(X_test.compute(), label=y_test.compute(), feature_names=feature_names)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# Set XGBoost parameters\n",
    "xgb_pars = {\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.5,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.9,\n",
    "    'lambda': 1.,\n",
    "    'nthread': -1, #here it uses all the available threads\n",
    "    'booster': 'gbtree',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=2, maximize=False, verbose_eval=1)\n",
    "print('Modeling RMSE %.5f' % model.best_score)\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(model, max_num_features=28, height=0.7)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = model.predict(dvalid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
